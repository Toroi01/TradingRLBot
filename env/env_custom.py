import random

import gym
import numpy as np
import pandas as pd
from gym import spaces
from stable_baselines3.common.vec_env import DummyVecEnv

from env.portfolio import Portfolio
from env.state import State


class CustomTradingEnv(gym.Env):
    """A custom stock trading environment for OpenAI gym"""
    metadata = {'render.modes': ['human']}

    def __init__(self, df, main_tickers, all_tickers, max_assets_amount_per_trade, technical_indicator_list,
                 initial_amount,
                 turbulence_threshold=None, reward_scaling=1, comission_value=None, transaction_price_threshold=None):
        # In each step, the maximum number of assets per ticker is limited to a value
        self.max_assets_amount_per_trade = max_assets_amount_per_trade
        # String list with the ticker names that we want to buy and sell
        self.main_tickers = [x.lower() for x in main_tickers]
        # String list with tickers that will be used as information.
        # Here we verify that main tickers are in both lists
        all_tickers = [x.lower() for x in all_tickers]
        all_tickers = list(set(all_tickers + self.main_tickers))
        self.all_tickers = all_tickers
        # Filter df with selected tickers
        self.df = df[df.tic.isin(self.all_tickers)]
        # Hyperparam value to multiply rewards
        self.reward_scaling = reward_scaling
        # Value between 0 and 1 that will represent the comission
        self.comission_value = comission_value
        # Threshold from a Uniform distribution to add noise
        self.turbulence_threshold = turbulence_threshold

        self.state = State(technical_indicator_list)
        self.portfolio = Portfolio(cash=initial_amount, ticker_list=self.main_tickers)
        self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.main_tickers),))
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,
                                            shape=(self.state.get_size(self.main_tickers, self.all_tickers),))

        self._hour_counter = -1
        self._episode = 0
        self._historic_allocation_amount = pd.DataFrame()
        self._historic_allocation_values = pd.DataFrame()

    def step(self, actions):
        """
        Step in the RL training process
        :param actions: Actions generated by the model. Shape 1 x number of stocks
        :return: New state, reward
        """
        # Update the hourly data and calcula the current portfolio value
        hourly_data = self._get_hourly_data()
        prev_portfolio_value = self.portfolio.get_total_portfolio_value(hourly_data)

        # Buy, sell or hold
        for ticker, action in zip(self.main_tickers, actions):
            self._do_action(ticker, action, hourly_data)

        # Update status with the new portfolio and data
        self.state.update(self.portfolio, hourly_data)

        # Calculate new portfolio value and the reward with the next hourtly data
        self._hour_counter += 1
        new_hourly_data = self._get_hourly_data()
        new_portfolio_value = self.portfolio.get_total_portfolio_value(new_hourly_data)
        reward = self._compute_reward(prev_portfolio_value, new_portfolio_value)

        # Logging current allocation
        self.log_allocation_amount(new_hourly_data)
        self.log_allocation_values(new_hourly_data)

        return self.state.values(), reward, self.is_done(), {}

    def _get_hourly_data(self):
        """
        Gets the data from the next timestep and updates the
        _hourly_counter variable. Filter by only the desired tickers.
        """
        if len(self.all_tickers) == 1:
            return self.df.iloc[self._hour_counter, :].to_frame().T
        return self.df.loc[self._hour_counter, :]

    def _do_action(self, ticker, action, hourly_data):
        """
        Perform action depending on the sign of it.
        0: Not buying nor selling
        >0: Buy
        <0: Sell
        If turbulence is present, sometimes we won't perform any action even if the algorithm wanted
        to.
        :param hourly_data: Data from the current hour for all the tickers
        :param ticker: String corresponding to the name of the ticker
        :param action: Value between 0 and 1
        :return:
        """
        asset_price = hourly_data.loc[hourly_data.tic == ticker, 'close'].values[0]
        timestamp = hourly_data['date'].iloc[0]
        amount = action * self.max_assets_amount_per_trade
        # Adding some noise not performing any action
        if self.turbulence_threshold and random.random() < self.turbulence_threshold:
            return
        if action == 0:
            # Not buying nor selling
            return
        elif action > 0:
            self._buy_stock(ticker, amount, asset_price, timestamp)
        else:
            self._sell_stock(ticker, amount, asset_price, timestamp)

    def _sell_stock(self, ticker, amount, price, timestamp):
        self.portfolio.sell(ticker, - amount, price, timestamp, comission_value=self.comission_value)

    def _buy_stock(self, ticker, amount, price, timestamp):
        self.portfolio.buy(ticker, amount, price, timestamp, comission_value=self.comission_value)

    def log_allocation_amount(self, hourly_data):
        portfolio_row = self.portfolio.to_df()
        portfolio_row['date'] = hourly_data.date.iloc[0]
        self._historic_allocation_amount = self._historic_allocation_amount.append(portfolio_row)

    def log_allocation_values(self, hourly_data):
        assets_values = {k: [v] for k, v in self.portfolio.get_assets_value(hourly_data).items()}
        portfolio_row = pd.DataFrame(assets_values)
        portfolio_row['date'] = hourly_data.date.iloc[0]
        self._historic_allocation_values = self._historic_allocation_values.append(portfolio_row)

    def _compute_reward(self, prev_portfolio_value, new_portfolio_value):
        """
        Computes difference in dollars between the portfolios before and after the step.
        Multiplies the profit by a scaling factor to be defined.
        :return: Profit scaled by reward_scaling
        """
        profit = new_portfolio_value - prev_portfolio_value
        return profit * self.reward_scaling

    def is_done(self):
        # Check if it's the last iteration
        return self._hour_counter == len(self.df) // len(self.all_tickers) - 1

    def reset(self):
        """
        Reseting the portfolio, state and the data used to feed the algorithm
        :return: Initial state
        """
        self.portfolio.reset()
        self._hour_counter = 0
        hourly_data = self._get_hourly_data()
        self.state.reset()
        self.state.update(self.portfolio, hourly_data)

        self._episode += 1
        self._historic_allocation_amount = pd.DataFrame()
        self._historic_allocation_values = pd.DataFrame()

        return self.state.values()

    def save_asset_memory(self):
        return self._historic_allocation_amount

    def save_action_memory(self):
        return self.portfolio.historic_transactions

    def save_asset_values_memory(self):
        return self._historic_allocation_values

    def render(self, mode='human', close=False):
        return self.state.values()

    def get_sb_env(self):
        e = DummyVecEnv([lambda: self])
        obs = e.reset()

        return e, obs
