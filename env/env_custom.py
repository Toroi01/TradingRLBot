import random

import gym
import numpy as np
import pandas as pd
from gym import spaces
from stable_baselines3.common.vec_env import DummyVecEnv
import sys
from env.portfolio import Portfolio
from env.state import State
from empyrical import sortino_ratio, calmar_ratio, sharpe_ratio

class CustomTradingEnv(gym.Env):
    """A custom stock trading environment for OpenAI gym"""
    metadata = {'render.modes': ['human']}

    def __init__(self, df, main_tickers, all_tickers, max_assets_amount_per_trade, technical_indicator_list,
                 initial_amount, reward_type = "absolute", reward_scaling=1,
                 turbulence_threshold=None,  comission_value=None, transaction_price_threshold=None
                ):
        # In each step, the maximum number of assets per ticker is limited to a value
        self.max_assets_amount_per_trade = max_assets_amount_per_trade
        # String list with the ticker names that we want to buy and sell
        self.main_tickers = [x.lower() for x in main_tickers]
        # String list with tickers that will be used as information.
        # Here we verify that main tickers are in both lists
        all_tickers = [x.lower() for x in all_tickers]
        all_tickers = list(set(all_tickers + self.main_tickers))
        self.all_tickers = all_tickers
        # Filter df with selected tickers
        self.df = df[df.tic.isin(self.all_tickers)]
        # Hyperparam value to decide the type of rewards
        self.reward_type = reward_type
        # Hyperparam value to multiply rewards
        self.reward_scaling = reward_scaling
        # Value between 0 and 1 that will represent the comission
        self.comission_value = comission_value
        # Threshold from a Uniform distribution to add noise
        self.turbulence_threshold = turbulence_threshold

        self.state = State(technical_indicator_list)
        self.portfolio = Portfolio(cash=initial_amount, ticker_list=self.main_tickers)
        self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.main_tickers),))
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,
                                            shape=(self.state.get_size(self.main_tickers, self.all_tickers),))

        self._hour_counter = -1
        self._episode = 0
        self._historic_allocation_amount = pd.DataFrame()
        self._historic_allocation_values = pd.DataFrame()

    def step(self, actions):
        """
        Step in the RL training process
        :param actions: Actions generated by the model. Shape 1 x number of stocks
        :return: New state, reward
        """
        # Update the hourly data and calcula the current portfolio value
        hourly_data = self._get_hourly_data()
        prev_portfolio_value = self.portfolio.get_total_portfolio_value(hourly_data)

        # Buy, sell or hold
        for ticker, action in zip(self.main_tickers, actions):
            self._do_action(ticker, action, hourly_data)

        # Update status with the new portfolio and data
        self.state.update(self.portfolio, hourly_data)

        # Calculate new portfolio value and the reward with the next hourtly data
        self._hour_counter += 1
        new_hourly_data = self._get_hourly_data()
        new_portfolio_value = self.portfolio.get_total_portfolio_value(new_hourly_data)
        reward = self._compute_reward(prev_portfolio_value, new_portfolio_value, self.reward_type, new_hourly_data)

        # Logging current allocation
        self.log_allocation_amount(new_hourly_data)
        self.log_allocation_values(new_hourly_data)
        return self.state.values(), reward, self.is_done(), {}

    def _get_hourly_data(self):
        """
        Gets the data from the next timestep and updates the
        _hourly_counter variable. Filter by only the desired tickers.
        """
        if len(self.all_tickers) == 1:
            return self.df.iloc[self._hour_counter, :].to_frame().T
        return self.df.loc[self._hour_counter, :]

    def _do_action(self, ticker, action, hourly_data):
        """
        Perform action depending on the sign of it.
        0: Not buying nor selling
        >0: Buy
        <0: Sell
        If turbulence is present, sometimes we won't perform any action even if the algorithm wanted
        to.
        :param hourly_data: Data from the current hour for all the tickers
        :param ticker: String corresponding to the name of the ticker
        :param action: Value between 0 and 1
        :return:
        """
        asset_price = hourly_data.loc[hourly_data.tic == ticker, 'close'].values[0]
        timestamp = hourly_data['date'].iloc[0]
        amount = action * self.max_assets_amount_per_trade
        # Adding some noise not performing any action
        if self.turbulence_threshold and random.random() < self.turbulence_threshold:
            return
        if action == 0:
            # Not buying nor selling
            return
        elif action > 0:
            self._buy_stock(ticker, amount, asset_price, timestamp)
        else:
            self._sell_stock(ticker, amount, asset_price, timestamp)

    def _sell_stock(self, ticker, amount, price, timestamp):
        self.portfolio.sell(ticker, - amount, price, timestamp, comission_value=self.comission_value)

    def _buy_stock(self, ticker, amount, price, timestamp):
        self.portfolio.buy(ticker, amount, price, timestamp, comission_value=self.comission_value)

    def log_allocation_amount(self, hourly_data):
        portfolio_row = self.portfolio.to_df()
        portfolio_row['date'] = hourly_data.date.iloc[0]
        self._historic_allocation_amount = self._historic_allocation_amount.append(portfolio_row)

    def log_allocation_values(self, hourly_data):
        assets_values = {k: [v] for k, v in self.portfolio.get_assets_value(hourly_data).items()}
        portfolio_row = pd.DataFrame(assets_values)
        portfolio_row['date'] = hourly_data.date.iloc[0]
        self._historic_allocation_values = self._historic_allocation_values.append(portfolio_row)

    def _compute_reward(self, prev_portfolio_value, new_portfolio_value, reward_type, hourly_data):
        """
        Computes the reward accordingly to the reward type specify
        """
        if reward_type == "absolute":
            reward = (new_portfolio_value - prev_portfolio_value) * self.reward_scaling
        elif reward_type == "percentage":
            reward= (new_portfolio_value/prev_portfolio_value)-1 
        elif reward_type in ["sharpe_ratio" , "sortino_ratio", "calmar_ratio"]:
            df_historic = self._historic_allocation_values
            assets_values = {k: [v] for k, v in self.portfolio.get_assets_value(hourly_data).items()}
            portfolio_row = pd.DataFrame(assets_values)
            portfolio_row['date'] = hourly_data.date.iloc[0]
            df_historic.append(portfolio_row)
            future_window = min(0, max(self.df.index)- self._hour_counter )
            if future_window >0:
                for i in range(future_window):
                    future_value = portfolio_row.copy()
                    close_future = self.df.loc[self._hour_counter + i+1].copy()
                    close_day =  self.df.loc[self._hour_counter].copy()
                    for j in self.main_tickers:
                        future_value[j] = future_value[j] * ( (close_future.loc[ close_future["tic"] == j]["close"] / close_day.loc[ close_day["tic"] == j ]["close"]) -1 )
                        if future_value[j] is None:
                            future_value[j] = 0
                    df_historic.append(future_value)
                
            if df_historic.shape[0] < 3:
                 reward = 0.2
            else:
                df_money = df_historic[["cash"]+ self.main_tickers]
                pct_change = df_money.sum(axis = 1).pct_change()
                window = min( df_money.shape[0] , 24)
                if reward_type == "sharpe_ratio":
                    reward = sharpe_ratio(pct_change[-window:])
                if reward_type == "sortino_ratio":
                    reward = sortino_ratio(pct_change[-window:])
                if reward_type == "calmar_ratio":
                    reward = calmar_ratio(pct_change[-window:])
        else:
            sys.exit("Unvalid reward")
        if np.isnan(reward):
                reward = 0
        return reward

    def is_done(self):
        # Check if it's the last iteration
        return self._hour_counter == len(self.df) // len(self.all_tickers) - 1

    def reset(self):
        """
        Reseting the portfolio, state and the data used to feed the algorithm
        :return: Initial state
        """
        self.portfolio.reset()
        self._hour_counter = 0
        hourly_data = self._get_hourly_data()
        self.state.reset()
        self.state.update(self.portfolio, hourly_data)

        self._episode += 1
        self._historic_allocation_amount = pd.DataFrame()
        self._historic_allocation_values = pd.DataFrame()

        return self.state.values()

    def save_asset_memory(self):
        return self._historic_allocation_amount

    def save_action_memory(self):
        return self.portfolio.historic_transactions

    def save_asset_values_memory(self):
        return self._historic_allocation_values

    def render(self, mode='human', close=False):
        return self.state.values()

    def get_sb_env(self):
        e = DummyVecEnv([lambda: self])
        obs = e.reset()

        return e, obs
